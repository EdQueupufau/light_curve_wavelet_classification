{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRSU5pPLRIfr"
      },
      "outputs": [],
      "source": [
        "!pip install lightkurve==2.0.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GfWBD_IJKfGJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, concatenate,Conv1D, Flatten,Dropout , BatchNormalization, MaxPooling1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzyoJ8VEt9ST"
      },
      "outputs": [],
      "source": [
        "print(tf.__version__)\n",
        "pickle.format_version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xwin1Ts_RROr"
      },
      "outputs": [],
      "source": [
        "# ruta de los archivos LightCurveWaveletCollection\n",
        "dataset_path= '/content/drive/MyDrive/MASTER IA (Unir)/TFM/waveletsK/'\n",
        "# ruta con el archivo csv con las estrllas seleccionadas\n",
        "df_path = '/content/drive/MyDrive/MASTER IA (Unir)/TFM/light_curves_K_stars_filter.csv'\n",
        "# punto de divicion entrenamiento y validacion (80%)\n",
        "train_split = .80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SYWcaeSgpx-Q"
      },
      "outputs": [],
      "source": [
        "class LightCurveWaveletFoldCollection():\n",
        "    \"\"\"\n",
        "      Clase que contiene las wavelets de las curvas de luz\n",
        "    \"\"\"\n",
        "    def __init__(self,light_curve,wavelets):\n",
        "        self._light_curve = light_curve\n",
        "        self._lc_w_collection = wavelets\n",
        "\n",
        "    def get_detail_coefficent(self,level = None):\n",
        "        \"\"\"\n",
        "        Metodo que devuelve el detalle de los coeficientes de las wavelets\n",
        "        Parameters\n",
        "        ----------\n",
        "        level: int = None\n",
        "          nivel de profunidad que se desea devolver.\n",
        "        Returns\n",
        "        ----------\n",
        "        retorna una Lista con los valores correspondientes al detalle del coeficiente.\n",
        "        \"\"\"\n",
        "        if level != None:\n",
        "            return self._lc_w_collection[level-1][1]\n",
        "        return self._lc_w_collection[:][1]\n",
        "\n",
        "    def get_approximation_coefficent(self,level = None):\n",
        "        \"\"\"\n",
        "         Metodo que devuelve el coeficiente de aproximacion de las wavelets.\n",
        "        Parameters\n",
        "        ----------\n",
        "        level: int = None\n",
        "          nivel de profunidad que se desea devolver.\n",
        "        Returns\n",
        "        ----------\n",
        "        retorna una Lista con los valores correspondientes al coeficiente de aproximacion.\n",
        "        \"\"\"\n",
        "        if level != None:\n",
        "            return self._lc_w_collection[level-1][0]\n",
        "        return self._lc_w_collection[:][0]\n",
        "    \n",
        "    def get_wavelets(self):\n",
        "        \"\"\"\n",
        "         Metodo que devuelve todas las wavelets.\n",
        "        Parameters\n",
        "        ----------\n",
        "        None.\n",
        "        Returns\n",
        "        ----------\n",
        "        retorna una Lista con todos los niveles de descomposicion wavelet.\n",
        "        \"\"\"\n",
        "        return self._lc_w_collection\n",
        "\n",
        "    def plot(self):\n",
        "        \"\"\"\n",
        "        Metodo imprime la curva de luz y sus descomposiciones.\n",
        "        Parameters\n",
        "        ----------\n",
        "        None.\n",
        "        Returns\n",
        "        ----------\n",
        "        None.\n",
        "        \"\"\"\n",
        "        wavelet = self._lc_w_collection\n",
        "        time = self._light_curve.time.value\n",
        "        data = self._light_curve.flux.value\n",
        "        plt.figure(figsize=(16, 4))\n",
        "        plt.plot(time,data)\n",
        "        ig, axarr = plt.subplots(nrows=len(wavelet), ncols=2, figsize=(16,12))\n",
        "        for i,lc_w in enumerate(wavelet):\n",
        "            (data, coeff_d) = lc_w\n",
        "            axarr[i, 0].plot(data, 'r')\n",
        "            axarr[i, 1].plot(coeff_d, 'g')\n",
        "            axarr[i, 0].set_ylabel(\"Level {}\".format(i + 1), fontsize=14, rotation=90)\n",
        "            axarr[i, 0].set_yticklabels([])\n",
        "            if i == 0:\n",
        "                axarr[i, 0].set_title(\"Approximation coefficients\", fontsize=14)\n",
        "                axarr[i, 1].set_title(\"Detail coefficients\", fontsize=14)\n",
        "            axarr[i, 1].set_yticklabels([])\n",
        "        plt.show()\n",
        "\n",
        "class LightCurveWaveletCollection():\n",
        "    def __init__(self,id,headers,lc_par,lc_inpar):\n",
        "        self.pliegue_par = lc_par\n",
        "        self.pliegue_inpar = lc_inpar\n",
        "        self.kepler_id = id\n",
        "        self.headers = headers\n",
        "\n",
        "    def save(self, path = \"\"):\n",
        "        \"\"\"\n",
        "        Metodo que guarda la curva luz procesada en un archivo.\n",
        "        Parameters\n",
        "        ----------\n",
        "        Path: str = \"\"\n",
        "          ruta donde se guardara el archivo.\n",
        "        Returns\n",
        "        ----------\n",
        "        None.\n",
        "        \"\"\"\n",
        "        file_name = path + 'kic '+str(self.kepler_id)+'.pickle'\n",
        "        with open(file_name, \"wb\") as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    def load(path):\n",
        "        \"\"\"\n",
        "        Metodo que carga la curva luz procesada en un archivo.\n",
        "        Parameters\n",
        "        ----------\n",
        "        Path: str = \"\"\n",
        "          ruta donde se encuentra almacenado el archivo.\n",
        "        Returns\n",
        "        ----------\n",
        "        w_loaded: LightCurveWaveletCollection\n",
        "          Clase que almacena las curvas de luz par e inpar ademas de metadata del proceso de descomposicion.\n",
        "        \"\"\"\n",
        "        with open(path, \"rb\") as f:\n",
        "            w_loaded = pickle.load(f)\n",
        "        return w_loaded\n",
        "\n",
        "    def plot_comparative(self):\n",
        "        \"\"\"\n",
        "        Metodo que imprime una comparativa entre la curva de luz par e impar..\n",
        "        Parameters\n",
        "        ----------\n",
        "        None.\n",
        "        Returns\n",
        "        ----------\n",
        "        None.\n",
        "        \"\"\"\n",
        "        light_curve_p = self.pliegue_par._light_curve\n",
        "        light_curve_i = self.pliegue_inpar._light_curve\n",
        "        w_par_Collection = self.pliegue_par\n",
        "        w_inpar_Collection = self.pliegue_inpar\n",
        "        wavelet_p=w_par_Collection.get_wavelets()\n",
        "        wavelet_i=w_inpar_Collection.get_wavelets()\n",
        "        plt.figure(figsize=(26, 8))\n",
        "        plt.plot(light_curve_p.time.value,light_curve_p.flux.value,c='blue',label='par')\n",
        "        plt.plot(light_curve_i.time.value,light_curve_i.flux.value,c='red',label='inpar')\n",
        "        \n",
        "        ig, axarr = plt.subplots(nrows=len(wavelet_p), ncols=2, figsize=(26,26))\n",
        "        for i,zip_curves in enumerate(zip(wavelet_p,wavelet_i)):\n",
        "            (data_p, coeff_p),(data_i, coeff_i) = zip_curves\n",
        "            axarr[i, 0].plot(data_p,c='blue',label='par')\n",
        "            axarr[i, 0].plot(data_i, c='red',label='inpar')\n",
        "            axarr[i, 1].plot(coeff_p, c='blue',label='par')\n",
        "            axarr[i, 1].plot(coeff_i, c='red',label='inpar')\n",
        "            axarr[i, 0].set_ylabel(\"Level {}\".format(i + 1), fontsize=14, rotation=90)\n",
        "            axarr[i, 0].set_yticklabels([])\n",
        "            if i == 0:\n",
        "                axarr[i, 0].set_title(\"Approximation coefficients\", fontsize=14)\n",
        "                axarr[i, 1].set_title(\"Detail coefficients\", fontsize=14)\n",
        "            axarr[i, 1].set_yticklabels([])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def fold_curve(light_curve_collection, period, epoch, sigma = 20, sigma_upper = 4):\n",
        "    \"\"\"\n",
        "    Toma la coleccion de la curvas entregadas, las pliega y devuelve una sola con todos los datos.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    light_curve_collection: LightCurveCollection\n",
        "        coleccion de curvas de luz.\n",
        "    period: float\n",
        "        periodo de la orbita.\n",
        "    epoch: float\n",
        "        tiempo de cada transcurso.\n",
        "    sigma: int\n",
        "        valor de desviaciones estandas\n",
        "    sigma_upper: int\n",
        "        valor maximo de variacion\n",
        "    Returns\n",
        "    ----------\n",
        "    una sola curva de luz\n",
        "    \"\"\"\n",
        "    if not is_colab:\n",
        "        lc_collection = lk.LightCurveCollection([lc.remove_outliers(sigma=20, sigma_upper=4) for lc in light_curve_collection])\n",
        "    \n",
        "    lc_ro = lc_collection.stitch()\n",
        "    \n",
        "    if is_colab:\n",
        "        lc_ro = lc_ro.remove_outliers(sigma=sigma, sigma_upper=sigma_upper)\n",
        "    \n",
        "    lc_nonans = lc_ro.remove_nans()\n",
        "    lc_fold = lc_nonans.fold(period = period,epoch_time = epoch)\n",
        "    lc_odd=lc_fold[lc_fold.odd_mask]\n",
        "    lc_even = lc_fold[lc_fold.even_mask]\n",
        "    return lc_fold,lc_odd,lc_even\n",
        "\n",
        "def apply_wavelet(light_curve,w_family, levels):\n",
        "    \"\"\"\n",
        "    Aplicacion de la wavelet a la curva de luz.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    light_curve: LightCurve\n",
        "        curva de luz a la que se le aplica la wavelet.\n",
        "    w_family: str\n",
        "        nombre de la familia de wavelet a aplicar.\n",
        "    levels: int > 0\n",
        "        niveles de profundida de aplicacion.\n",
        "    Returns\n",
        "    ----------\n",
        "    lc_wavelet: [(numpy.array,numpy.array)]\n",
        "        Lista con los coeficientes de aproximacion y coeficiente de detalle.\n",
        "    \"\"\"\n",
        "    time = light_curve.time.value\n",
        "    data = light_curve.flux.value\n",
        "    lc_wavelet = []\n",
        "    for level in range(levels):\n",
        "        level_w = pywt.dwt(data, w_family)\n",
        "        lc_wavelet.append(level_w)\n",
        "        data = level_w[0]\n",
        "    return LightCurveWaveletFoldCollection(light_curve,lc_wavelet)\n",
        "\n",
        "def load_light_curve(kepler_id,mission='Kepler'):\n",
        "    \"\"\"\n",
        "    Descarga de la curva de luz a patir de la mision y el id.\n",
        "    Parameters\n",
        "    ----------\n",
        "    kepler_id: int\n",
        "        identificador Kepler.\n",
        "    mission: str = 'Kepler' \n",
        "        mision que obtuvo el regiostro.\n",
        "    Returns\n",
        "    ----------\n",
        "    lc_collection: LightCurveCollection\n",
        "        Coleccion de curvas de luz.\n",
        "    \"\"\"\n",
        "    kic = 'KIC '+str(kepler_id)\n",
        "    lc_search = lk.search_lightcurve(kic, mission=mission)\n",
        "    lc_collection = lc_search.download_all()\n",
        "    return lc_collection\n",
        "\n",
        "def cut_wavelet(lightCurve,window):\n",
        "    \n",
        "    time = lightCurve.time\n",
        "    data = lightCurve.flux\n",
        "    flux_error = lightCurve.flux_err\n",
        "    index = np.argmin(np.absolute(time))\n",
        "    min_w = index - int(window/2)\n",
        "    max_w = index + int(window/2)+1\n",
        "    time_selected = time[min_w:max_w]\n",
        "    data_selected = data[min_w:max_w]\n",
        "    flux_error_selected = flux_error[min_w:max_w]\n",
        "    return lk.lightcurve.FoldedLightCurve(time=time_selected,flux=data_selected,flux_err=flux_error_selected)\n",
        "    \n",
        "def process_light_curve(kepler_id,disp,period,epoch,w_family,levels,plot = False, plot_comparative=False,save=False, path=\"\",wavelet_window=None):\n",
        "    \"\"\"\n",
        "    Metodo que procesa la curva de luz para obtener sus transformadas wavelet.\n",
        "    Parameters\n",
        "    ----------\n",
        "    kepler_id: int\n",
        "        identificador Kepler.\n",
        "    period: float\n",
        "        valor correspondiente al periodo de la curva.   \n",
        "    epoch: float\n",
        "        valor correspondiente a la epoca de la curva. \n",
        "    w_family: str\n",
        "        familia de wavelet a aplicar. \n",
        "    levels: int\n",
        "        niveles de profundida para aplicar las wavelets\n",
        "    plot: boolean = False\n",
        "        imprime una grafica con los resultados de las wavelets\n",
        "    plot_comparative: boolean = False\n",
        "        imprime una grafica comparativa entre las wavelets de los diferentes niveles en curvas pares e inpares.\n",
        "    save: boolean = False\n",
        "        genera un archivo guardando la curva de luz y sus descomposiciones wavelets\n",
        "    path: str\n",
        "        ruta donde se guardara el archivo en caso de que el parametro save sea True\n",
        "    wavelet_window: int\n",
        "        tamaño de la ventana para recortar la curva de luz plegada\n",
        "    cut_border_percent: float\n",
        "        porcentaje del total de puntos a recortar del total existente en el nivel de la descomposicion wavelet.\n",
        "    Returns\n",
        "    ----------\n",
        "    lc_wavelet_collection: LightCurveWaveletCollection\n",
        "        Objeto que contiene la curva de luz y sus wavelets ademas de una cabecera con metadata del proceso.\n",
        "    \"\"\"\n",
        "    # cargamos la curva de segun su Kepler_ID\n",
        "    print(\"descargando curvas de luz...\")\n",
        "    lc_collection=load_light_curve(kepler_id)\n",
        "    # aplicamos el pliege a las curvas de luz y las separamos en pares e inpares\n",
        "    print('Aplicando pliegue y separando en pares e inpares....') \n",
        "    _,lc_inpar,lc_par = fold_curve(lc_collection,period,epoch)\n",
        "\n",
        "    if not wavelet_window == None:\n",
        "      print('Aplicando ventana ...')\n",
        "      lc_inpar = cut_wavelet(lc_inpar,wavelet_window)\n",
        "      lc_par = cut_wavelet(lc_par,wavelet_window)\n",
        "    \n",
        "    print('Aplicando wavelets...')\n",
        "    # aplicamos wavelets a curvas par\n",
        "    lc_w_par = apply_wavelet(lc_par,w_family,levels)\n",
        "    # aplicamos wavelets a curvas inpar\n",
        "    lc_w_inpar = apply_wavelet(lc_inpar,w_family,levels)\n",
        "    headers = {\n",
        "        \"period\": period,\n",
        "        \"epoch\": epoch,\n",
        "        \"class\": disp,\n",
        "        \"wavelet_family\":w_family,\n",
        "        \"levels\":levels,\n",
        "        \"window\":wavelet_window\n",
        "    }\n",
        "    lc_wavelet_collection = LightCurveWaveletCollection(kepler_id,headers,lc_w_par,lc_w_inpar)\n",
        "    if(plot):\n",
        "        print('graficando wavelets obtenidas...')\n",
        "        lc_w_par.plot()\n",
        "        lc_w_inpar.plot()\n",
        "    if(plot_comparative):\n",
        "        print('graficando wavelets obtenidas...')\n",
        "        lc_wavelet_collection.plot_comparative()\n",
        "    if(save):\n",
        "        print('guardando wavelets obtenidas...')\n",
        "        lc_wavelet_collection.save(path)\n",
        "    return lc_wavelet_collection\n",
        "\n",
        "def process_dataset(df_koi,plot = False, plot_comparative = False,repeat_completed=True,completed=None):\n",
        "    \"\"\"\n",
        "    Metodo que procesa el dataframe de curvas de luz para obtener sus transformadas wavelet.\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_koi: pandas.DataFrame\n",
        "        Dataset con los datos de las curvas de luz a procesar.\n",
        "    plot: boolean = False\n",
        "        Imprime las curvas de luz y sus descomposiciones.\n",
        "    plot_comparative: boolean = False:\n",
        "        Imprime una comparativa entre la curva de luz par e impar con sus descomposiciones\n",
        "    repeat_completed: boolean = True\n",
        "        Reprocesa las curvas de luz que ya se encuentran procesadas y almacenadas. (Se ocupa para recalcular curvas de procesos anteriores incompletos)\n",
        "    completed: List = None\n",
        "        Lista con las curvas de luz completadas de proceso anteriores. \n",
        "    Returns\n",
        "    ----------\n",
        "    lc_errors: List <int>\n",
        "        Lista con las curvas de luz que no se pudieron procesar.\n",
        "    \"\"\"\n",
        "    lc_wavelets = dict()\n",
        "    lc_errors = []\n",
        "    for i in range (len(df_koi)):\n",
        "\n",
        "        koi_id,disp, period, epoch=df_koi[['kepid','koi_disposition','koi_period','koi_time0bk']].iloc[i]\n",
        "        percent = i*100/(len(df_koi))\n",
        "        print(f'procesando curva de luz KIC {int(koi_id)}[{disp}] [{percent:.0f}%]')\n",
        "        if not repeat_completed and str(koi_id) in completed:\n",
        "          print(\"curva de luz procesada anteriormente\")\n",
        "          continue\n",
        "        try:\n",
        "             process_light_curve(int(koi_id),disp,period,epoch,wavelet_family,level,plot= plot,plot_comparative=plot_comparative,save = save_lc, path = save_path,wavelet_window=wavelet_windows)\n",
        "        except:\n",
        "            lc_errors.append(koi_id)\n",
        "            print(f'Error with KIC {koi_id}')\n",
        "    f = open (save_path+'errors.txt','w')\n",
        "    for lc_error in lc_errors:\n",
        "        text = 'KIC '+str(lc_error)+'\\n'\n",
        "        f.write(text)\n",
        "    f.close()\n",
        "    return lc_errors\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pzak6wvKQ2VS"
      },
      "outputs": [],
      "source": [
        "def plot_results(history):\n",
        "    \"\"\"\n",
        "    Imprime un grafico con los valores de acurray y loss para entrenamiento y test.\n",
        "    Parameters\n",
        "    ----------\n",
        "    histoy: List\n",
        "        valores de entrenmiento y perdida obtenidas de la funcion fit.\n",
        "    Returns\n",
        "    ----------\n",
        "    None.\n",
        "    \"\"\"\n",
        "    # GRÁFICO DE LA PRECISIÓN y PERDIDA CON DATOS DE ENTRENAMIENTO\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.plot(epochs, acc, 'r', label='Presición Entrenamiento')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Presición Validación')\n",
        "    plt.title('Presición entrenamiento y test')\n",
        "    plt.legend(loc=0)\n",
        "    plt.figure()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epochs, loss, 'r',linestyle = 'dashed', label='Pérdida de Entrenamiento')\n",
        "    plt.plot(epochs, val_loss, 'b',linestyle = 'dashed', label='Perdida de Validación')\n",
        "    plt.title('Pérdida entrenamiento y test')\n",
        "    plt.legend(loc=0)\n",
        "    plt.show()\n",
        "    \n",
        "def load_files(path,randomize=True):\n",
        "    \"\"\"\n",
        "    Genera una lista con los archivos existentes de las curvas de luz procesadas.\n",
        "    Parameters\n",
        "    ----------\n",
        "    path: str\n",
        "      carpeta donde se encuentran almacendas las curvas de luz.\n",
        "    randomize: boolean = True\n",
        "        desordena las posiciones del los nombres de curvas de luz cargadas.\n",
        "    Returns\n",
        "    ----------\n",
        "    completed_id: List\n",
        "      Una lista con los identificadores de los archivos encontrados.\n",
        "    \"\"\"\n",
        "    completed = os.listdir(path)\n",
        "    if 'errors.txt' in completed:\n",
        "      completed.remove('errors.txt')\n",
        "    completed_id = []\n",
        "    for element in completed:\n",
        "      completed_id.append(path+element)\n",
        "    if randomize:\n",
        "      random.shuffle(completed_id)\n",
        "    return completed_id\n",
        "\n",
        "def generate_dataset_model_1(path,level=1):\n",
        "    \"\"\"\n",
        "    Genera el dataset con las curvas de luz cargadas, este metodo solo devuelve 1 nivel de curvas de luz.\n",
        "    Parameters\n",
        "    ----------\n",
        "    path: str\n",
        "      carpeta donde se encuentran almacendas las curvas de luz.\n",
        "    level: int = 1\n",
        "        nivel de profundidad de la descomposicion wavelet a cargar.\n",
        "    Returns\n",
        "    ----------\n",
        "    dataset_par: List\n",
        "      Lista con las curvas pares\n",
        "    dataset_inpar: List\n",
        "      Lista con las curvas impares\n",
        "    labels: List\n",
        "      Lista con los Labeles correspondientes a cada curva de luz.\n",
        "    \"\"\"\n",
        "    files = load_files(path)\n",
        "    dataset_par =[]\n",
        "    dataset_inpar= []\n",
        "    labels = []\n",
        "    len_points = None\n",
        "\n",
        "    for i,file in enumerate(files):\n",
        "      output.clear()\n",
        "      print(f\"loading [{i*100/len(files):.0f}%] file:{file}\")\n",
        "      lcwC = LightCurveWaveletCollection.load(file)\n",
        "      status = lcwC.headers['class']\n",
        "      curva_par = lcwC.pliegue_par.get_approximation_coefficent(level=level)\n",
        "      curva_inpar = lcwC.pliegue_inpar.get_approximation_coefficent(level=level)\n",
        "      #print(i,len(curva_par),len(curva_inpar),['-' for x in range(int(len(curva_par)/10))])\n",
        "      if len_points == None:\n",
        "        len_points = len(curva_par)\n",
        "      if len(curva_par)!= len_points or  len(curva_inpar)!= len_points:\n",
        "        continue\n",
        "      dataset_par.append(curva_par)\n",
        "      #dataset_par=np.append(dataset_par,[curva_par])\n",
        "      dataset_inpar.append(curva_inpar)\n",
        "      #dataset_inpar=np.append(dataset_inpar,[curva_inpar])\n",
        "      labels.append(0 if status == 'FALSE POSITIVE' else 1)\n",
        "    \n",
        "    dataset_par = np.array(dataset_par)\n",
        "    dataset_inpar = np.array(dataset_inpar)\n",
        "    labels = np.array(labels)\n",
        "    return dataset_par,dataset_inpar,labels\n",
        "\n",
        "def generate_dataset_model_2(path,levels=[1],show_loading = True):\n",
        "    \"\"\"\n",
        "    Genera el dataset con las curvas de luz cargadas, este metodo es capas de devolver mas de 1 nivel de curvas de luz.\n",
        "    Parameters\n",
        "    ----------\n",
        "    path: str\n",
        "      carpeta donde se encuentran almacendas las curvas de luz.\n",
        "    levels: List = [1]\n",
        "        nivel de profundidad de la descomposicion wavelet a cargar.\n",
        "    show_loading: boolean = True\n",
        "      Muestra el porcentaje de carga de las curvas de luz.\n",
        "    Returns\n",
        "    ----------\n",
        "    dataset_par: List\n",
        "      Lista con las curvas pares\n",
        "    dataset_inpar: List\n",
        "      Lista con las curvas impares\n",
        "    labels: List\n",
        "      Lista con los Labeles correspondientes a cada curva de luz.\n",
        "    \"\"\"\n",
        "    files = load_files(path)\n",
        "    #print(f\"file len:{len(files)}\")\n",
        "    labels = []\n",
        "    len_points = {}\n",
        "    curvas = {}\n",
        "\n",
        "    for level in levels:\n",
        "      curvas[\"par_\"+str(level)] = []\n",
        "      curvas[\"impar_\"+str(level)] = []\n",
        "      len_points[str(level)]=None\n",
        "\n",
        "    for i,file in enumerate(files):\n",
        "      skip_label = False\n",
        "      if show_loading:\n",
        "        print(f\"loading [{i*100/len(files):.0f}%] file:{file}\")\n",
        "      lcwC = LightCurveWaveletCollection.load(file)\n",
        "      status = lcwC.headers['class']\n",
        "\n",
        "      for level in levels:\n",
        "        curva_par = lcwC.pliegue_par.get_approximation_coefficent(level=level)\n",
        "        curva_inpar = lcwC.pliegue_inpar.get_approximation_coefficent(level=level)\n",
        "\n",
        "        if len_points[str(level)] == None:\n",
        "          len_points[str(level)] = len(curva_par)\n",
        "        if len(curva_par)!= len_points[str(level)] or  len(curva_inpar)!= len_points[str(level)]:\n",
        "          skip_label = True\n",
        "          break\n",
        "        curvas[\"par_\"+str(level)].append(curva_par)\n",
        "        curvas[\"impar_\"+str(level)].append(curva_inpar)\n",
        "\n",
        "      if not skip_label:\n",
        "        labels.append(0 if status == 'FALSE POSITIVE' else 1)\n",
        "    #dataset_par = np.array(dataset_par)\n",
        "    #dataset_inpar = np.array(dataset_inpar)\n",
        "    for level in levels:\n",
        "      curvas[\"par_\"+str(level)] = np.array(curvas[\"par_\"+str(level)])\n",
        "      curvas[\"impar_\"+str(level)] = np.array(curvas[\"impar_\"+str(level)])\n",
        "    labels = np.array(labels)\n",
        "    #print(\"len curvas\",len(curvas[\"par_\"+str(levels[0])]),  \" len labels\", len(labels) )\n",
        "    return curvas,labels\n",
        "\n",
        "\n",
        "def split_dataset(dataset_p, dataset_i, labels, split=.80):\n",
        "    \"\"\"\n",
        "    Metodo que divide el dataset en entrenamiento y test.\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_p: List\n",
        "      Lista con las wavelets pares.\n",
        "    dataset_i: List \n",
        "      Lista con las wavelets impares.\n",
        "    labels: List\n",
        "      Etiquetas de la wavelet correspondientes.\n",
        "    split: float = .80\n",
        "      porcentaje de registros donde se realizara el corte correspondientes a entrenamiento y test\n",
        "    Returns\n",
        "      List_1: List [List,List]\n",
        "        lista con las curvas pares e impares para entrenamiento \n",
        "      List_2: List [List,List]\n",
        "        lista con las curvas pares e impares para test\n",
        "      y_train: List [0|1]\n",
        "        Lista con las etiquetas de las curvas de entrenamiento\n",
        "      y_test: List [0|1]\n",
        "        Lista con las etiquetas de las curvas de test\n",
        "    ----------\n",
        "    dataset_par: List\n",
        "      Lista con las curvas pares\n",
        "    dataset_inpar: List\n",
        "      Lista con las curvas impares\n",
        "    labels: List\n",
        "      Lista con los Labeles correspondientes a cada curva de luz.\n",
        "    \"\"\"\n",
        "    split = int(len(labels)*split)\n",
        "    print(f\"before par:{np.shape(dataset_p)} impar:{np.shape(dataset_i)}, labels:{len(labels)}\")\n",
        "    X_p_train = dataset_p[:split]\n",
        "    X_i_train = dataset_i[:split]\n",
        "    y_train = labels[:split]\n",
        "\n",
        "    X_p_test = dataset_p[split:]\n",
        "    X_i_test = dataset_i[split:]\n",
        "    y_test = labels[split:]\n",
        "\n",
        "    X_p_train = np.expand_dims(X_p_train, axis=-1)\n",
        "    X_i_train = np.expand_dims(X_i_train, axis=-1)\n",
        "    X_p_test = np.expand_dims(X_p_test, axis=-1)\n",
        "    X_i_test = np.expand_dims(X_i_test, axis=-1)\n",
        "    print(f\"par:{np.shape(X_p_test)} impar:{np.shape(X_i_test)}, labels:{len(y_test)}\")\n",
        "    return [X_p_train, X_i_train], [X_p_test, X_i_test], y_train, y_test\n",
        "\n",
        "def normalize_data(data):\n",
        "    \"\"\"\n",
        "    Metodo que normaliza los datos de una lista.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: List\n",
        "      lista con los valores de la curva par o impar.\n",
        "    Returns\n",
        "    ----------\n",
        "    value: List\n",
        "      lista con los valores normalizados entre 0 y 1\n",
        "    \"\"\"\n",
        "    min = np.min(data)\n",
        "    max = np.max(data)\n",
        "    return (data - min)/(max-min) \n",
        "\n",
        "def normalize_data_2(data_p,data_i):\n",
        "    \"\"\"\n",
        "    Metodo que normaliza los datos de 2 lista en conjunto.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_p: List\n",
        "      lista con los valores de la curva par\n",
        "    data_p: List\n",
        "      lista con los valores de la curva impar \n",
        "    Returns\n",
        "    ----------\n",
        "    list_1: List\n",
        "      lista con los valores normalizados entre 0 y 1 de la wavelet par\n",
        "    list_2: List\n",
        "      lista con los valores normalizados entre 0 y 1 de la wavelet impar\n",
        "    \"\"\"\n",
        "    min = np.min(data_p) if np.min(data_p) < np.min(data_i) else np.min(data_i)\n",
        "    max = np.max(data_p) if np.max(data_p) > np.max(data_i) else np.max(data_i)\n",
        "    return [(data_p - min)/(max-min) , (data_i - min)/(max-min)]\n",
        "\n",
        "def normalize_LC(curvas_dic):\n",
        "    \"\"\"\n",
        "    Metodo que aplica la normalizacion a multiples niveles de descomposicion wavelet.\n",
        "    Parameters\n",
        "    ----------\n",
        "    curvas_dic: List\n",
        "      diccionario que contiene los distintos niveles de curvas.\n",
        "    Returns\n",
        "    ----------\n",
        "    list: List [List[List],List[list]] => List[wavelt_par[registros],wavelet_impar[registros]]\n",
        "      lista con los valores normalizados entre 0 y 1 de todos los niveles de wavelet\n",
        "    \"\"\"\n",
        "    #return [ [normalize_data(curvas_dic[ list(curvas_dic.keys())[i]]),normalize_data(curvas_dic[ list(curvas_dic.keys())[i+1]]) ] for i in range(0,len(curvas_dic.keys()),2)   ] \n",
        "    return [ normalize_data_2(curvas_dic[ list(curvas_dic.keys())[i]],curvas_dic[ list(curvas_dic.keys())[i+1]])  for i in range(0,len(curvas_dic.keys()),2)   ] \n",
        "\n",
        "def split_data_list(list_data,labels):\n",
        "    \"\"\"\n",
        "      Metodo que divide en entrenamiento y test los registros de mas de una curva wavelet.\n",
        "      Parameters\n",
        "      ----------\n",
        "      list_data: List\n",
        "        Lista que contiene los distintos niveles de curvas.\n",
        "      labels: List\n",
        "        Lista que contiene las etiquetas correspondientes a las descomposiciones wavelet.\n",
        "      Returns\n",
        "      ----------\n",
        "      ds_train: List\n",
        "        Lista con los registros para entrenamiento\n",
        "      ds_test: List\n",
        "        Lista con los registros para validacion\n",
        "      label_train: List\n",
        "        Lista con las etiquetas para entrenamiento\n",
        "      label_test: List\n",
        "        Lista con las etiquetas para validacion\n",
        "      \"\"\"\n",
        "    ds_train = []\n",
        "    ds_test = []\n",
        "    label_train = []\n",
        "    label_test = []\n",
        "    first = True\n",
        "    for c_par, c_impar in list_data:\n",
        "      X_train, X_test, y_train, y_test = split_dataset(c_par, c_impar,labels)\n",
        "      ds_train.append(X_train)\n",
        "      ds_test.append(X_test)\n",
        "      if first :\n",
        "        label_train = y_train\n",
        "        label_test = y_test\n",
        "        first = False\n",
        "\n",
        "    return ds_train,ds_test,label_train,label_test\n",
        "\n",
        "def evaluate_model(model,dataset,verbose = 0,epochs=1000):\n",
        "    \"\"\"\n",
        "    Metodo que evalua un modelo de multiple niveles de descomposicion wavelet y muestra los resultados de entrenamiento y validacion.\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: def\n",
        "      nombre de la funcion generadora del modelo a probar.\n",
        "    dataset: str\n",
        "      path con la ubicacion de las LightCurveWaveletCollection.\n",
        "    Verbose: int = 0\n",
        "      nivel de informacion del entrenamiento, mas informacion en la web de Keras.\n",
        "    epochs: int = 1000\n",
        "      cantidad de epocas de entrenamiento.\n",
        "    Returns\n",
        "    ----------\n",
        "    None.\n",
        "    \"\"\"\n",
        "    # Modelado Dataset\n",
        "    print('normalizando datos...')\n",
        "    ds_levels = normalize_LC(dataset[0])\n",
        "    print('dividiendo datos en entrenamioento y test...')\n",
        "    ds_train,ds_test,label_train,label_test = split_data_list(ds_levels,dataset[1])\n",
        "    # entrenamiento\n",
        "    print('generando modelo...')\n",
        "    model_g = model(ds_train,activation = tf.keras.layers.LeakyReLU())\n",
        "    print('compilando modelo....')\n",
        "    model_g.compile(loss = 'binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy','binary_crossentropy'])\n",
        "    print('entrenando....')\n",
        "    #early_stopping_acc = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.0005, patience=30, mode='max', verbose = 1)#EarlyStopping(monitor='accuracy', patience=15, min_delta=0.005, mode='max')\n",
        "    early_stopping_loss = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0005, patience=15, mode='min', verbose = 1)\n",
        "    history = model_g.fit(ds_train, label_train, epochs=epochs, batch_size=64,validation_split=0.20,shuffle=True,verbose=verbose, callbacks=[early_stopping_loss])\n",
        "    print('obteniendo resultados..')\n",
        "    plot_results(history)\n",
        "    result = model_g.evaluate(ds_test, label_test)\n",
        "    print(result)\n",
        "    del model_g, history, result, ds_train,ds_test,label_train, label_test, ds_levels\n",
        "  \n",
        "def evaluate_model_1_level(model,dataset,level,verbose = 0 ,epochs=1000):\n",
        "    \"\"\"\n",
        "    Metodo que evalua un modelo de un solo nivel de descomposicion wavelet y muestra los resultados de entrenamiento y validacion.\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: def\n",
        "      nombre de la funcion generadora del modelo a probar.\n",
        "    dataset: str\n",
        "      path con la ubicacion de las LightCurveWaveletCollection.\n",
        "    level: int\n",
        "      niveles de wavelet a evaluar.\n",
        "    Verbose: int = 0\n",
        "      nivel de informacion del entrenamiento, mas informacion en la web de Keras.\n",
        "    epochs: int = 1000\n",
        "      cantidad de epocas de entrenamiento.\n",
        "    Returns\n",
        "    ----------\n",
        "    None.\n",
        "    \"\"\"\n",
        "    print('Cargando dataset...')\n",
        "    ds_p,ds_i,label = generate_dataset_model_1(dataset_path,level)\n",
        "    print('normalizando datos...')\n",
        "    ds = normalize_data_2(ds_p,ds_i)\n",
        "    print('dividiendo dataset...')\n",
        "    X_train, X_test, y_train, y_test = split_dataset(ds[0], ds[1], label)\n",
        "    print('generando modelo....')\n",
        "    model_g = model(X_train,activation = tf.keras.layers.LeakyReLU())\n",
        "    model_g.compile(loss = 'binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "    print('entrenando...')\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, min_delta=0.005, mode='max')\n",
        "    #early_stopping_loss = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0005, patience=15, mode='min', verbose = 1)\n",
        "    history_2 = model_g.fit(X_train, y_train, epochs=epochs, batch_size=64,validation_split=0.20,shuffle=True,verbose = verbose, callbacks=[early_stopping])\n",
        "    print('Resultados...')\n",
        "    plot_results(history_2)\n",
        "    _, accuracy = model_g.evaluate(X_test, y_test)\n",
        "    print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "    y_prediction =[0 if x <= 0.5 else 1 for x in model_g.predict(X_test) ]\n",
        "    result = confusion_matrix(y_test, y_prediction)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=result)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "    del  X_train, X_test, y_train, y_test, ds_p, ds_i, ds, label, model_g, history_2, accuracy\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QdUCI1qmkzS"
      },
      "source": [
        "# Construnccion del modelo 1\n",
        "    [Conv1D(16,5)]    [Conv1D(16,5)]\n",
        "    [Conv1D(16,5)]    [Conv1D(16,5)]\n",
        "    [MaxPool(3,2)]    [MaxPool(3,2)]\n",
        "    [Conv1D(32,5)]    [Conv1D(32,5)]\n",
        "    [Conv1D(32,5)]    [Conv1D(32,5)]\n",
        "    [MaxPool(3,2)]    [MaxPool(3,2)]\n",
        "      [Flatten()]      [Flatten()]\n",
        "                [Concat]\n",
        "            [Dense(512relu)]\n",
        "            [Dense(512relu)]\n",
        "            [Dense(512relu)]\n",
        "            [Dense(512relu)]\n",
        "            [Dense(1sigmoid)]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3QQJ5W2d4gfZ"
      },
      "outputs": [],
      "source": [
        "def gen_model_1_level(ds,activation = 'relu'): \n",
        "  input_shape = np.shape(ds)[2:]\n",
        "  print(input_shape)\n",
        "  model_p = tf.keras.Sequential()\n",
        "  model_p.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=input_shape))\n",
        "  model_p.add(Conv1D(filters=16, kernel_size=5, activation='relu')) \n",
        "  model_p.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "  model_p.add(Conv1D(32,5, activation='relu'))\n",
        "  model_p.add(Conv1D(32,5, activation='relu'))\n",
        "  model_p.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "  model_p.add(Flatten())\n",
        "\n",
        "  model_i = tf.keras.Sequential()\n",
        "  model_i.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=input_shape))\n",
        "  model_i.add(Conv1D(filters=16, kernel_size=5, activation='relu')) \n",
        "  model_i.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "  model_i.add(Conv1D(32,5, activation='relu'))\n",
        "  model_i.add(Conv1D(32,5, activation='relu'))\n",
        "  model_i.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "  model_i.add(Flatten())\n",
        "\n",
        "  model_f = concatenate([model_p.output,model_i.output], axis=-1)\n",
        "  model_f = Dense(512,activation='relu')(model_f)\n",
        "  model_f = Dense(512,activation='relu')(model_f)\n",
        "  model_f = Dense(512,activation='relu')(model_f)\n",
        "  model_f = Dense(512,activation='relu')(model_f)\n",
        "  model_f = Dense(1,activation='sigmoid')(model_f)\n",
        "  model_f = Model([model_p.input,model_i.input],model_f)\n",
        "  return model_f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siPYumaAruqp"
      },
      "source": [
        "# Construnccion del modelo 1 nivel con 1 seccion convolucional mas\n",
        "    [Conv1D(16,5)]    [Conv1D(16,5)]\n",
        "    [Conv1D(16,5)]    [Conv1D(16,5)]\n",
        "    [MaxPool(3,2)]    [MaxPool(3,2)]\n",
        "    [Conv1D(32,5)]    [Conv1D(32,5)]\n",
        "    [Conv1D(32,5)]    [Conv1D(32,5)]\n",
        "    [MaxPool(3,2)]    [MaxPool(3,2)]\n",
        "    [Conv1D(64,5)]    [Conv1D(64,5)]\n",
        "    [Conv1D(64,5)]    [Conv1D(64,5)]\n",
        "    [MaxPool(3,2)]    [MaxPool(3,2)]\n",
        "      [Flatten()]      [Flatten()]\n",
        "                [Concat]\n",
        "            [Dense(512relu)]\n",
        "            [Dense(512relu)]\n",
        "            [Dense(512relu)]\n",
        "            [Dense(512relu)]\n",
        "            [Dense(1sigmoid)]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Obg9qPdIrr4-"
      },
      "outputs": [],
      "source": [
        "def gen_model_1_level_2(ds,activation = 'relu'): \n",
        "  input_shape = np.shape(ds)[2:]\n",
        "  model_p = tf.keras.Sequential()\n",
        "  model_p.add(Conv1D(filters=32, kernel_size=5, input_shape=input_shape))\n",
        "  model_p.add(Conv1D(filters=32, kernel_size=5, )) \n",
        "  model_p.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "  model_p.add(Conv1D(64,5))\n",
        "  model_p.add(Conv1D(64,5))\n",
        "  model_p.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "  model_p.add(Conv1D(128,5))\n",
        "  model_p.add(Conv1D(128,5))\n",
        "  model_p.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "  model_p.add(Flatten())\n",
        "\n",
        "  model_i = tf.keras.Sequential()\n",
        "  model_i.add(Conv1D(filters=32, kernel_size=5,  input_shape=input_shape))\n",
        "  model_i.add(Conv1D(filters=32, kernel_size=5,)) \n",
        "  model_i.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "  model_i.add(Conv1D(64,5))\n",
        "  model_i.add(Conv1D(64,5))\n",
        "  model_i.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "  model_i.add(Conv1D(128,5))\n",
        "  model_i.add(Conv1D(128,5))\n",
        "  model_i.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "  model_i.add(Flatten())\n",
        "\n",
        "  model_f = concatenate([model_p.output,model_i.output], axis=-1)\n",
        "  model_f = BatchNormalization(axis=-1)(model_f)\n",
        "  model_f = Dense(256,activation='relu')(model_f)\n",
        "  model_f = Dense(256,activation='relu')(model_f)\n",
        "  model_f = Dropout(.2)(model_f)\n",
        "  model_f = Dense(256,activation='relu')(model_f)\n",
        "  model_f = Dense(256,activation='relu')(model_f)\n",
        "  model_f = Dropout(.2)(model_f)\n",
        "  model_f = Dense(256,activation='relu')(model_f)\n",
        "  model_f = Dense(1,activation='sigmoid')(model_f)\n",
        "  model_f_2 = Model(inputs=[model_p.input,model_i.input],outputs=model_f)\n",
        "  return model_f_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_dLcwI7tiwV"
      },
      "source": [
        "# Construnccion del modelo 2 niveles\n",
        "\n",
        "    [Conv1D(16,5)]                                        [Conv1D(16,5)]\n",
        "    [Conv1D(16,5)]                                        [Conv1D(16,5)]\n",
        "    [MaxPool(3,1)]                                        [MaxPool(3,1)]\n",
        "    [Conv1D(32,5)]    [Conv1D(16,5)]    [Conv1D(16,5)]    [Conv1D(32,5)]\n",
        "    [Conv1D(32,5)]    [Conv1D(16,5)]    [Conv1D(16,5)]    [Conv1D(32,5)]\n",
        "    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\n",
        "    [Conv1D(64,5)]    [Conv1D(32,5)]    [Conv1D(32,5)]    [Conv1D(64,5)]\n",
        "    [Conv1D(64,5)]    [Conv1D(32,5)]    [Conv1D(32,5)]    [Conv1D(64,5)]\n",
        "    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\n",
        "    [Conv1D(128,5)]   [Conv1D(64,5)]    [Conv1D(64,5)]    [Conv1D(128,5)]\n",
        "    [Conv1D(128,5)]   [Conv1D(64,5)]    [Conv1D(64,5)]    [Conv1D(128,5)]\n",
        "    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\n",
        "      [Flatten()]      [Flatten()]        [Flatten()]      [Flatten()]\n",
        "                                  [Concat]\n",
        "                              [Dense(512relu)]\n",
        "                              [Dense(512relu)]\n",
        "                              [Dense(512relu)]\n",
        "                              [Dense(512relu)]\n",
        "                              [Dense(1sigmoid)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDtSmJr_vLSJ"
      },
      "outputs": [],
      "source": [
        "def gen_model_2_levels(ds,activation = 'relu',summary=False):\n",
        "    # tamaño nivel 7\n",
        "    input_shape_1 = np.shape(ds[0])[2:]\n",
        "    # tamaño nivel 8\n",
        "    input_shape_2 = np.shape(ds[1])[2:]\n",
        "   \n",
        "    # rama par level 7\n",
        "    model_p_7 = tf.keras.Sequential()\n",
        "    model_p_7.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_1))\n",
        "    model_p_7.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Flatten())\n",
        "    \n",
        "    # rama par level 8\n",
        "    model_p_8 = tf.keras.Sequential()\n",
        "    model_p_8.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_2))\n",
        "    model_p_8.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_8.add(Flatten())\n",
        "    \n",
        "    # rama impar level 7\n",
        "    model_i_7 = tf.keras.Sequential()\n",
        "    model_i_7.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_1))\n",
        "    model_i_7.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Flatten())\n",
        "    \n",
        "    # rama impar level 8\n",
        "    model_i_8 = tf.keras.Sequential()\n",
        "    model_i_8.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_2))\n",
        "    model_i_8.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_8.add(Flatten())\n",
        "\n",
        "    # Red profunda\n",
        "    model_f = concatenate([model_p_7.output,model_i_7.output,\n",
        "                           model_p_8.output,model_i_8.output], axis=-1)\n",
        "    model_f = BatchNormalization(axis=-1)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(1,activation='sigmoid')(model_f)\n",
        "    model_f = Model([[model_p_7.input,model_i_7.input],[model_p_8.input,model_i_8.input]],model_f)\n",
        "    if summary:\n",
        "      model_f.summary()\n",
        "    return model_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "53s0EX4Ex4S_",
        "outputId": "b5f91e89-6ac8-4cde-be1f-e014faef9800"
      },
      "outputs": [],
      "source": [
        "model_3 =  gen_model_3()\n",
        "model_3.compile(loss = 'binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy','binary_crossentropy'])\n",
        "history_2 = model_3.fit([X_train_7,X_train_8], y_train_7, epochs=1000, batch_size=64,validation_split=0.15,shuffle=True)\n",
        "plot_results(history_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMs4doFD4Tcr"
      },
      "source": [
        "# Construnccion del modelo 3 niveles\n",
        "\n",
        "\t\t[Conv1D(16,5)]\t\t\t\t                                          \t\t\t\t\t[Conv1D(16,5)]\n",
        "\t\t[Conv1D(16,5)]\t\t\t\t\t                                        \t\t\t\t[Conv1D(16,5)]\n",
        "\t\t[MaxPool(3,1)]\t\t\t\t\t                                        \t\t\t\t[MaxPool(3,1)]\n",
        "\t\t[Conv1D(32,5)]\t[Conv1D(16,5)]                                        [Conv1D(16,5)]\t[Conv1D(32,5)]\n",
        "\t\t[Conv1D(32,5)]\t[Conv1D(16,5)]                                        [Conv1D(16,5)]\t[Conv1D(32,5)]\n",
        "\t\t[MaxPool(3,5)]\t[MaxPool(3,1)]                                        [MaxPool(3,1)]\t[MaxPool(3,1)]\n",
        "\t\t[Conv1D(64,5)]\t[Conv1D(32,5)]    [Conv1D(16,5)]    [Conv1D(16,5)]    [Conv1D(32,5)]\t[Conv1D(64,5)]\n",
        "\t\t[Conv1D(64,5)]\t[Conv1D(32,5)]    [Conv1D(16,5)]    [Conv1D(16,5)]    [Conv1D(32,5)]\t[Conv1D(64,5)]\n",
        "\t\t[MaxPool(3,5)]\t[MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\t[MaxPool(3,1)]\n",
        "\t\t[Conv1D(128,5)]\t[Conv1D(64,5)]    [Conv1D(32,5)]    [Conv1D(32,5)]    [Conv1D(64,5)]\t[Conv1D(128,5)]\n",
        "\t\t[Conv1D(12u,5)]\t[Conv1D(64,5)]    [Conv1D(32,5)]    [Conv1D(32,5)]    [Conv1D(64,5)]\t[Conv1D(128,5)]\n",
        "\t\t[MaxPool(3,5)]\t[MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\t[MaxPool(3,1)]\n",
        "\t\t[Conv1D(256,5)]\t[Conv1D(128,5)]   [Conv1D(64,5)]    [Conv1D(64,5)]    [Conv1D(128,5)]\t[Conv1D(256,5)]\n",
        "\t\t[Conv1D(256,5)]\t[Conv1D(128,5)]   [Conv1D(64,5)]    [Conv1D(64,5)]    [Conv1D(128,5)]\t[Conv1D(256,5)]\n",
        "\t\t[MaxPool(3,1)]\t[MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\t[MaxPool(16,5)]\n",
        "\t\t [Flatten()]\t  [Flatten()]      [Flatten()]        [Flatten()]      [Flatten()]\t\t[Flatten()]\n",
        "                                                        [Concat]\n",
        "                                                      [Dense(512relu)]\n",
        "                                                      [Dense(512relu)]\n",
        "                                                      [Dense(512relu)]\n",
        "                                                      [Dense(512relu)]\n",
        "                                                      [Dense(1sigmoid)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNrOjswE65dz"
      },
      "outputs": [],
      "source": [
        "def gen_model_3_levels(ds,activation = 'relu', print_summary = False):\n",
        "    # tamaño nivel 3\n",
        "    input_shape_3 = np.shape(ds[0])[2:]\n",
        "    # tamaño nivel 7\n",
        "    input_shape_7 = np.shape(ds[1])[2:]\n",
        "    # tamaño nivel 8\n",
        "    input_shape_8 = np.shape(ds[2])[2:]\n",
        "   \n",
        "     # rama par level 3\n",
        "    model_p_3 = tf.keras.Sequential()\n",
        "    model_p_3.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_3))\n",
        "    model_p_3.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_3.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_3.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_3.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_3.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_3.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_3.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_3.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_3.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_3.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_3.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_3.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_3.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_3.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_3.add(Flatten())\n",
        "\n",
        "    # rama par level 7\n",
        "    model_p_7 = tf.keras.Sequential()\n",
        "    model_p_7.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_7))\n",
        "    model_p_7.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Flatten())\n",
        "    \n",
        "    # rama par level 8\n",
        "    model_p_8 = tf.keras.Sequential()\n",
        "    model_p_8.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_8))\n",
        "    model_p_8.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_8.add(Flatten())\n",
        "    \n",
        "     # rama impar level 3\n",
        "    model_i_3 = tf.keras.Sequential()\n",
        "    model_i_3.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_3))\n",
        "    model_i_3.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_3.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_3.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_3.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_3.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_3.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_3.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_3.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_3.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_3.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_3.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_3.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_3.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_3.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_3.add(Flatten())\n",
        "\n",
        "    # rama impar level 7\n",
        "    model_i_7 = tf.keras.Sequential()\n",
        "    model_i_7.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_7))\n",
        "    model_i_7.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Flatten())\n",
        "    \n",
        "    # rama impar level 8\n",
        "    model_i_8 = tf.keras.Sequential()\n",
        "    model_i_8.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_8))\n",
        "    model_i_8.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_8.add(Flatten())\n",
        "\n",
        "    # Red profunda\n",
        "    model_f = concatenate([model_p_3.output,model_i_3.output,\n",
        "                           model_p_7.output,model_i_7.output,\n",
        "                           model_p_8.output,model_i_8.output], axis=-1)\n",
        "    \n",
        "    model_f = BatchNormalization(axis=-1)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dropout(0.2)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dropout(0.2)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(1,activation='sigmoid')(model_f)\n",
        "    model_f = Model([[model_p_3.input,model_i_3.input],[model_p_7.input,model_i_7.input],[model_p_8.input,model_i_8.input]],model_f)\n",
        "    if print_summary:\n",
        "      model_f.summary()\n",
        "    return model_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK2nPmCM-L6b"
      },
      "outputs": [],
      "source": [
        "model_4 =  gen_model_3_levels(ds_3_train,activation = tf.keras.layers.LeakyReLU())\n",
        "model_4.compile(loss = 'binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy','binary_crossentropy'])\n",
        "history_4 = model_4.fit(ds_3_train, label_3_train, epochs=1000, batch_size=64,validation_split=0.15,shuffle=True)\n",
        "plot_results(history_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn2umUMDODBd"
      },
      "source": [
        "# Construnccion modelo de 4 niveles\n",
        "\n",
        "\t\t[Conv1D(16,5)]\t\t\t\t                                          \t\t\t\t\t[Conv1D(16,5)]\n",
        "\t\t[Conv1D(16,5)]\t\t\t\t\t                                        \t\t\t\t[Conv1D(16,5)]\n",
        "\t\t[MaxPool(3,1)]\t\t\t\t\t                                        \t\t\t\t[MaxPool(3,1)]\n",
        "\t\t[Conv1D(32,5)]\t[Conv1D(16,5)]                                        [Conv1D(16,5)]\t[Conv1D(32,5)]\n",
        "\t\t[Conv1D(32,5)]\t[Conv1D(16,5)]                                        [Conv1D(16,5)]\t[Conv1D(32,5)]\n",
        "\t\t[MaxPool(3,5)]\t[MaxPool(3,1)]                                        [MaxPool(3,1)]\t[MaxPool(3,1)]\n",
        "\t\t[Conv1D(64,5)]\t[Conv1D(32,5)]    [Conv1D(16,5)]    [Conv1D(16,5)]    [Conv1D(32,5)]\t[Conv1D(64,5)]\n",
        "\t\t[Conv1D(64,5)]\t[Conv1D(32,5)]    [Conv1D(16,5)]    [Conv1D(16,5)]    [Conv1D(32,5)]\t[Conv1D(64,5)]\n",
        "\t\t[MaxPool(3,5)]\t[MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\t[MaxPool(3,1)]\n",
        "\t\t[Conv1D(128,5)]\t[Conv1D(64,5)]    [Conv1D(32,5)]    [Conv1D(32,5)]    [Conv1D(64,5)]\t[Conv1D(128,5)]\n",
        "\t\t[Conv1D(12u,5)]\t[Conv1D(64,5)]    [Conv1D(32,5)]    [Conv1D(32,5)]    [Conv1D(64,5)]\t[Conv1D(128,5)]\n",
        "\t\t[MaxPool(3,5)]\t[MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\t[MaxPool(3,1)]\n",
        "\t\t[Conv1D(256,5)]\t[Conv1D(128,5)]   [Conv1D(64,5)]    [Conv1D(64,5)]    [Conv1D(128,5)]\t[Conv1D(256,5)]\n",
        "\t\t[Conv1D(256,5)]\t[Conv1D(128,5)]   [Conv1D(64,5)]    [Conv1D(64,5)]    [Conv1D(128,5)]\t[Conv1D(256,5)]\n",
        "\t\t[MaxPool(3,1)]\t[MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]    [MaxPool(3,1)]\t[MaxPool(16,5)]\n",
        "\t\t [Flatten()]\t  [Flatten()]      [Flatten()]        [Flatten()]      [Flatten()]\t\t[Flatten()]\n",
        "                                                        [Concat]\n",
        "                                                      [Dense(512relu)]\n",
        "                                                      [Dense(512relu)]\n",
        "                                                      [Dense(512relu)]\n",
        "                                                      [Dense(512relu)]\n",
        "                                                      [Dense(1sigmoid)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_hld4M3ODXu"
      },
      "outputs": [],
      "source": [
        "def gen_model_4_levels(ds_train,activation = 'relu'):\n",
        "    # tamaño nivel 5\n",
        "    input_shape_5 = np.shape(ds_train[0])[2:]\n",
        "    # tamaño nivel 6\n",
        "    input_shape_6 = np.shape(ds_train[1])[2:]\n",
        "    # tamaño nivel 7\n",
        "    input_shape_7 = np.shape(ds_train[2])[2:]\n",
        "    # tamaño nivel 8\n",
        "    input_shape_8 = np.shape(ds_train[3])[2:]\n",
        "\n",
        "     # rama par level 5\n",
        "    model_p_5 = tf.keras.Sequential()\n",
        "    model_p_5.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_5))\n",
        "    model_p_5.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_5.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_5.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_5.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_5.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_5.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_5.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_5.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_5.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_5.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_5.add(Conv1D(256,5, activation=activation))\n",
        "    model_p_5.add(Conv1D(256,5, activation=activation))\n",
        "    model_p_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_5.add(Conv1D(256,5, activation=activation))\n",
        "    model_p_5.add(Conv1D(256,5, activation=activation))\n",
        "    model_p_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_5.add(Flatten())\n",
        "\n",
        "    # rama par level 6\n",
        "    model_p_6 = tf.keras.Sequential()\n",
        "    model_p_6.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_6))\n",
        "    model_p_6.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_6.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_6.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_6.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_6.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_6.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_6.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_6.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_6.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_6.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_6.add(Conv1D(256,5, activation=activation))\n",
        "    model_p_6.add(Conv1D(256,5, activation=activation))\n",
        "    model_p_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_6.add(Flatten())\n",
        "\n",
        "    # rama par level 7\n",
        "    model_p_7 = tf.keras.Sequential()\n",
        "    model_p_7.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_7))\n",
        "    model_p_7.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_p_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_7.add(Flatten())\n",
        "    \n",
        "    # rama par level 8\n",
        "    model_p_8 = tf.keras.Sequential()\n",
        "    model_p_8.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_8))\n",
        "    model_p_8.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_p_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_p_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_p_8.add(Flatten())\n",
        "    \n",
        "     # rama impar level 5\n",
        "    model_i_5 = tf.keras.Sequential()\n",
        "    model_i_5.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_5))\n",
        "    model_i_5.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_5.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_5.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_5.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_5.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_5.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_5.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_5.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_5.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_5.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_5.add(Conv1D(256,5, activation=activation))\n",
        "    model_i_5.add(Conv1D(256,5, activation=activation))\n",
        "    model_i_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_5.add(Conv1D(256,5, activation=activation))\n",
        "    model_i_5.add(Conv1D(256,5, activation=activation))\n",
        "    model_i_5.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_5.add(Flatten())\n",
        "\n",
        "    # rama impar level 6\n",
        "    model_i_6 = tf.keras.Sequential()\n",
        "    model_i_6.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_6))\n",
        "    model_i_6.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_6.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_6.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_6.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_6.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_6.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_6.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_6.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_6.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_6.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_6.add(Conv1D(256,5, activation=activation))\n",
        "    model_i_6.add(Conv1D(256,5, activation=activation))\n",
        "    model_i_6.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_6.add(Flatten())\n",
        "\n",
        "    # rama impar level 7\n",
        "    model_i_7 = tf.keras.Sequential()\n",
        "    model_i_7.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_7))\n",
        "    model_i_7.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_7.add(Conv1D(128,5, activation=activation))\n",
        "    model_i_7.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_7.add(Flatten())\n",
        "    \n",
        "    # rama impar level 8\n",
        "    model_i_8 = tf.keras.Sequential()\n",
        "    model_i_8.add(Conv1D(16, 5, activation=activation, input_shape=input_shape_8))\n",
        "    model_i_8.add(Conv1D(16, 5, activation=activation)) \n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1)) \n",
        "    model_i_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_8.add(Conv1D(32,5, activation=activation))\n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_8.add(Conv1D(64,5, activation=activation))\n",
        "    model_i_8.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "    model_i_8.add(Flatten())\n",
        "\n",
        "    # Red profunda\n",
        "    model_f = concatenate([model_p_5.output,model_i_5.output,\n",
        "                           model_p_6.output,model_i_6.output,\n",
        "                           model_p_7.output,model_i_7.output,\n",
        "                           model_p_8.output,model_i_8.output], axis=-1)\n",
        "    model_f = Dense(1012,activation=activation)(model_f)\n",
        "    model_f = Dense(750,activation=activation)(model_f)\n",
        "    model_f = Dropout(0.2)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dropout(0.2)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(512,activation=activation)(model_f)\n",
        "    model_f = Dense(1,activation='sigmoid')(model_f)\n",
        "    model_f = Model([[model_p_5.input,model_i_5.input],[model_p_6.input,model_i_6.input],[model_p_7.input,model_i_7.input],[model_p_8.input,model_i_8.input]],model_f)\n",
        "    model_f.summary()\n",
        "    return model_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JethGx4LOFeD"
      },
      "outputs": [],
      "source": [
        "model_5 =  gen_model_5(ds_4_train, activation = tf.keras.layers.LeakyReLU())\n",
        "model_5.compile(loss = 'binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy','binary_crossentropy'])\n",
        "history_5 = model_5.fit(ds_4_train, label_4_train, epochs=1000, batch_size=64,validation_split=0.15,shuffle=True)\n",
        "plot_results(history_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBYOWvTCfLun"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-xK7bDeHIV"
      },
      "source": [
        "# Modelo de prueba de conv1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mr-4L5ywtxq3",
        "outputId": "3fe7bd3f-d741-4e7d-f757-816f9cb9fa36"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(Conv1D(filters=16, kernel_size=5, activation='sigmoid', input_shape=(59, 1)))\n",
        "model.add(Conv1D(filters=16, kernel_size=5, activation='sigmoid')) \n",
        "model.add(MaxPooling1D(pool_size=3, strides=2)) # change to 2 or add `padding=\"same\"` to the conv layers\n",
        "model.add(Conv1D(32,5, activation='sigmoid'))\n",
        "model.add(Conv1D(32,5, activation='sigmoid'))\n",
        "model.add(MaxPooling1D(pool_size=3, strides=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "hist_test = model.fit(X_train_8[0], y_train_8, epochs=50, batch_size=32,validation_split=0.3,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhMbJnqndEG2"
      },
      "outputs": [],
      "source": [
        "plot_results(hist_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAEoL5Vscyvz"
      },
      "outputs": [],
      "source": [
        "_, accuracy = model.evaluate(X_test[0], y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di4M9N1qa9p2"
      },
      "source": [
        "# Pruebas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0uqYYm40Mdd"
      },
      "source": [
        "## Curvas de luz solas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ucJZ5vtCOakP"
      },
      "outputs": [],
      "source": [
        "def generate_dataset_lc(path,reduce_point = None):\n",
        "  files = load_files(path)\n",
        "  lc_dateset_par = []\n",
        "  lc_dateset_impar = []\n",
        "  lc_labels = []\n",
        "  len_lc_par = None\n",
        "  len_lc_impar = None\n",
        "  is_first = True\n",
        "  for i in files:\n",
        "    lc = LightCurveWaveletCollection.load(i)\n",
        "    lc_class = lc.headers['class']\n",
        "    lc_par = lc.pliegue_par._light_curve.flux.value\n",
        "    lc_impar = lc.pliegue_inpar._light_curve.flux.value\n",
        "    if reduce_point != None:  \n",
        "      lc_par = lc_par[reduce_point:(len(lc_par)-reduce_point)] \n",
        "      lc_impar = lc_impar[reduce_point:(len(lc_impar)-reduce_point)]\n",
        "    if is_first:\n",
        "      is_first = False\n",
        "      len_lc_par = len(lc_par)\n",
        "    len_lc_impar = len(lc_impar)\n",
        "    if len(lc_par) == len_lc_par and len(lc_impar) == len_lc_impar:\n",
        "      lc_dateset_par.append(lc_par)\n",
        "      lc_dateset_impar.append(lc_impar)\n",
        "      lc_labels.append(1 if lc_class =='CONFIRMED' else 0)\n",
        "  dataset_par = np.array(lc_dateset_par)\n",
        "  dataset_inpar = np.array(lc_dateset_impar)\n",
        "  lc_labels = np.array(lc_labels)\n",
        "  return [lc_dateset_par,lc_dateset_impar],lc_labels\n",
        "\n",
        "def normalize_lc_lc(dataset):\n",
        "  ds_norm_p = []\n",
        "  ds_norm_i = []\n",
        "  for lc_p, lc_i in zip(dataset[0],dataset[1]):\n",
        "    lc_p_n, lc_i_n = normalize_data_2(lc_p,lc_i)\n",
        "    ds_norm_p.append(lc_p_n)\n",
        "    ds_norm_i.append(lc_i_n)\n",
        "  return [ds_norm_p,ds_norm_i]\n",
        "lc_dateset,lc_labels = generate_dataset_lc(dataset_path,reduce_point=4000)\n",
        "lc_dateset_norm = normalize_lc_lc(lc_dateset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "tIF__Ik9TZMg",
        "outputId": "6a8898a8-1061-497f-af09-7820e40083ae"
      },
      "outputs": [],
      "source": [
        "display(np.shape(lc_dateset))\n",
        "display(np.shape(lc_dateset_norm))\n",
        "ds_lc_train,ds_lc_test,label_lc_train,label_lc_test = split_dataset(lc_dateset_norm[0], lc_dateset_norm[1],lc_labels)\n",
        "display(np.shape(ds_lc_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a23yz4-aCxI",
        "outputId": "985a6c1b-e46c-4aca-a55e-6a32205407c6"
      },
      "outputs": [],
      "source": [
        "type(label_lc_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a08HKzi_UtP4",
        "outputId": "d48e67db-0b7c-4dd4-98d7-58803cd68109"
      },
      "outputs": [],
      "source": [
        "model_lc = gen_model_1_level(ds_lc_train,activation = tf.keras.layers.LeakyReLU())\n",
        "model_lc.compile(loss = 'binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "print('entrenando...')\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=30, min_delta=0.005, mode='max')\n",
        "\n",
        "history_2 = model_lc.fit(ds_lc_train, label_lc_train, epochs=1000, batch_size=64,validation_split=0.15,shuffle=True,verbose = 2, callbacks=[early_stopping])\n",
        "print('Resultados...')\n",
        "plot_results(history_2)\n",
        "_, accuracy = model_lc.evaluate(ds_lc_test, label_lc_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "y_prediction =[0 if x <= 0.5 else 1 for x in model_lc.predict(ds_lc_test) ]\n",
        "result = confusion_matrix(label_lc_test, y_prediction)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=result)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twRMz1xvkoDl"
      },
      "source": [
        "## 1 nivel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRvu1y3CVXsj",
        "outputId": "9a35d7b5-221a-4c68-de8d-0de28bee10e2"
      },
      "outputs": [],
      "source": [
        "evaluate_model_1_level(gen_model_1_level_2,dataset_path,2,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "tyudQ5b8rac0",
        "outputId": "44120991-277b-401e-8adf-13d3edcf10d3"
      },
      "outputs": [],
      "source": [
        "evaluate_model_1_level(gen_model_1_level_2,dataset_path,3,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kn-k-vbcttI9",
        "outputId": "135c85a6-0231-4cd6-ff6d-7edfc8e2af87"
      },
      "outputs": [],
      "source": [
        "evaluate_model_1_level(gen_model_1_level_2,dataset_path,4,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ImBI2j2x7sr3",
        "outputId": "4950260b-25bb-45bb-d073-268ad70a96e9"
      },
      "outputs": [],
      "source": [
        "evaluate_model_1_level(gen_model_1_level_2,dataset_path,5,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbb0qyKcAM4Q"
      },
      "outputs": [],
      "source": [
        "evaluate_model_1_level(gen_model_1_level_2,dataset_path,6,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxShttO3AYSL"
      },
      "outputs": [],
      "source": [
        "evaluate_model_1_level(gen_model_1_level_2,dataset_path,7,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hvlcqjhDDe7M",
        "outputId": "b0f0961a-33a4-4b8c-90b0-eb3d7ba62245"
      },
      "outputs": [],
      "source": [
        "evaluate_model_1_level(gen_model_1_level_2,dataset_path,8,verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnqI_BVoG_aV"
      },
      "source": [
        "## 2 niveles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1OSauS2eHFng",
        "outputId": "2bfa12af-f37b-4dbf-d1de-4e50ec7490eb"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_2_levels,generate_dataset_model_2(dataset_path,levels=[3,4],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q5f1CSASHFhv",
        "outputId": "aa4fe3f9-9af4-4042-b946-56c6d16d2bda"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_2_levels,generate_dataset_model_2(dataset_path,levels=[4,5],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qIhRxWgFHFSL",
        "outputId": "6dcf0c9e-63b8-498a-adc9-3d392e10acbc"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_2_levels,generate_dataset_model_2(dataset_path,levels=[5,6],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "V5aZHs2IHLja",
        "outputId": "14112159-9818-4c0f-fa2d-f299b030e7a2"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_2_levels,generate_dataset_model_2(dataset_path,levels=[6,7],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bORjsYFTHM5R",
        "outputId": "be7189e9-45a0-460a-e938-af21213f46f0"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_2_levels,generate_dataset_model_2(dataset_path,levels=[7,8],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikhrOF3sbeyD"
      },
      "source": [
        "## 3 niveles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0tQzptJ-BC6",
        "outputId": "71824cdd-4405-4952-a8ee-1378b7ce7235"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_3_levels,generate_dataset_model_2(dataset_path,levels=[3,4,5],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-zd8oRkV6qP"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_3_levels,generate_dataset_model_2(dataset_path,levels=[4,5,6],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1YhLxoihlP1C",
        "outputId": "1be72d79-6e50-468d-b577-88374dcb0aab"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_3_levels,generate_dataset_model_2(dataset_path,levels=[5,6,7],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OnsF0JLfgvj4",
        "outputId": "1196c870-8ce5-4914-b102-c41a957d5786"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_3_levels,generate_dataset_model_2(dataset_path,levels=[6,7,8],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWE1yEDFktnI"
      },
      "source": [
        "## 4 niveles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uHB05l1jkwf6",
        "outputId": "44003222-da5a-4362-8d9f-1789aaa44694"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_4_levels,generate_dataset_model_2(dataset_path,levels=[5,6,7,8],show_loading=False),verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsD2v26m6_9G",
        "outputId": "7754e11d-c6e6-4041-8f16-8e26a704a2c3"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gen_model_4_levels,generate_dataset_model_2(dataset_path,levels=[4,5,6,7],show_loading=False),verbose=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4QdUCI1qmkzS",
        "g_dLcwI7tiwV",
        "CMs4doFD4Tcr",
        "jn2umUMDODBd",
        "8A-xK7bDeHIV",
        "twRMz1xvkoDl",
        "XnqI_BVoG_aV",
        "ikhrOF3sbeyD",
        "GWE1yEDFktnI"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
